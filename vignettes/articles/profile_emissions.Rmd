---
title: "profile_emissions.Rmd"
output: github_document
params:
  chunks: NULL
  order: "sample"
  cache_dir: NULL
  input: "input"
  output: "output"
  europages_companies: "europages_companies.csv"
  ecoinvent_activities: "ecoinvent_activities.csv"
  ecoinvent_europages: "ecoinvent_europages.csv"
  isic: "isic.csv"
  emissions_profile_any_companies: "emissions_profile_any_companies.csv"
  emissions_profile_products: "emissions_profile_products.csv"
---

<!-- This file is generated from inst/extdata. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This workflow runs in chunks of the `*companies` data and caches intermediate
results. This saves memory, completes faster, and allows you to resume after
interruptions.

## Setup

```{r global}
library(dplyr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(rappdirs)
library(future)
library(fs)

# Masking `tiltIndicatorAfter::profile*()` to use `chunks`
library(tiltWorkflows)
```

If the parameter `chunks` is `NULL` (default) your `*companies` dataset is
automatically chunked to distribute its companies across available cores. This
uses your computer resources efficiently but may not be enough. Consider
adjusting the `chunks` parameter manually. Aim to balance memory-usage and
speed. A small number of `chunks` makes each chunk bigger and may overwhelm your
memory. A large number of `chunks` may take longer because of the overhead of
caching each chunk.

<details>
<summary>Parameters</summary>

```{r}
params
```

</details>

```{r}
options(
  # Determines the number of chunks
  tiltWorkflows.chunks = params$chunks,
  # Determines the order in which the chunks run
  tiltWorkflows.order = params$order,
  # Determines where to store the cache
  tiltWorkflows.cache_dir = params$cache_dir,
  # Read data quietly
  readr.show_col_types = FALSE,
  # Make printed output wider
  width = 500
)

# Enable computing over multiple workers in parallel
plan(multisession)

# Ensure input/ and output/ directories
if (!dir_exists(params$input)) use_toy_input()
if (!dir_exists(params$output)) dir_create(params$output)
```

<details>
<summary>Session information</summary>

```{r}
getwd()

availableCores()

dir_tree(params$input)

dir_tree(params$output)

sessioninfo::session_info()
```

</details>
## Data

This example defaults to using toy datasets but you may [use the
parameters](https://2degreesinvesting.github.io/tiltWorkflows/articles/tiltWorkflows.html)
of this file to instead use your own data.

```{r}
europages_companies <- read_csv(path(params$input, params$europages_companies)) |>
  FIXME_issue_49()
ecoinvent_activities <- read_csv(path(params$input, params$ecoinvent_activities))
ecoinvent_europages <- read_csv(path(params$input, params$ecoinvent_europages))
isic <- read_csv(path(params$input, params$isic))
```
Data specific to this indicator.

```{r}
emissions_profile_any_companies <- read_csv(path(params$input, params$emissions_profile_any_companies))
# FIXME User toy_emissions_profile_products_ecoinvent()
# See https://github.com/2DegreesInvesting/tiltToyData/pull/12
# https://github.com/2DegreesInvesting/tiltWorkflows/issues/9
emissions_profile_products <- read_csv(path(params$input, params$emissions_profile_products))
```

## Emissions profile

`r explain_indicator()`

```{r}
emissions_profile <- profile_emissions(
  companies = emissions_profile_any_companies,
  co2 = emissions_profile_products,
  europages_companies = europages_companies,
  ecoinvent_activities = ecoinvent_activities,
  ecoinvent_europages = ecoinvent_europages,
  isic = isic,
  low_threshold = 1 / 3,
  high_threshold = 2 / 3
)
```

## Results

`r explain_results()`

```{r}
emissions_profile |>
  unnest_product() |>
  print() |>
  write_csv(path(params$output, "emissions_profile_at_product_level.csv"))

emissions_profile |>
  unnest_company() |>
  print() |>
  write_csv(path(params$output, "emissions_profile_at_company_level.csv"))
```

The results at product and company level are now saved in the output/ directory.

```{r}
# NOTE: If other workflows run before this one, this shows the results of all
params$output |> dir_tree()
```
## Cleanup

Here is the cache that allows you to resume after interruptions. 

* The number of files is determined by `params$chunks`.

```{r eval=dir_exists(user_cache_dir("tiltWorkflows"))}
# NOTE: If other workflows run before this one, this shows the cache of all
dir_tree(user_cache_dir("tiltWorkflows"))
```

If you want to recompute some result, you must first delete the relevant cache:

```r
library(fs)
library(rappdirs)

dir_delete(user_cache_dir("tiltWorkflows/PROFILE-DIRECTORY-YOU-WANT-TO-DELETE"))
```
