```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This workflow runs in chunks of the `*companies` data and caches intermediate
results. This saves memory, completes faster, and allows you to resume after
interruptions.

## Setup

```{r global}
library(dplyr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(rappdirs)
library(future)
library(fs)

# Masking `tiltIndicatorAfter::profile*()` to use `chunks`
library(tiltWorkflows)
```

If the parameter `chunks` is `NULL` (default) your `*companies` dataset is
automatically chunked to distribute its companies across available cores. This
uses your computer resources efficiently but may not be enough. Consider
adjusting the `chunks` parameter manually. Aim to balance memory-usage and
speed. A small number of `chunks` makes each chunk bigger and may overwhelm your
memory. A large number of `chunks` may take longer because of the overhead of
caching each chunk.

<details>
<summary>Parameters</summary>

```{r}
params
```

</details>

```{r}
options(
  tiltWorkflows.chunks = params$chunks,
  tiltWorkflows.order = params$order,
  tiltWorkflows.cache_dir = params$cache_dir,
  # Read data quietly
  readr.show_col_types = FALSE,
  # Make printed output wider
  width = 500
)

# Enable computing over multiple workers in parallel
plan(multisession)

# Ensure input/ and output/ directories
if (!dir_exists(params$input)) use_toy_input()
if (!dir_exists(params$output)) dir_create(params$output)
```

<details>
<summary>Session information</summary>

```{r}
getwd()

availableCores()

dir_tree(params$input)

dir_tree(params$output)

sessioninfo::session_info()
```

</details>

